{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "import json\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ot7iZput-x",
        "outputId": "8040c17f-41f0-432a-c96c-4151e439ef35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "rEMxhoNMvA05"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsmLhViyu9Rn",
        "outputId": "37d278b8-dc3a-4a06-dc4c-2023cf64599b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m194.6/209.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Function"
      ],
      "metadata": {
        "id": "tPIRypk7uj6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"slang_words.txt\", \"r\") as slang_file:\n",
        "  slang_content = slang_file.read()\n",
        "  slang_words = ast.literal_eval(slang_content)"
      ],
      "metadata": {
        "id": "h67rEp0SYn60"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EKoeFyH0uSqm"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "  text = re.sub('-',' ',text)\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text\n",
        "\n",
        "def case_folding(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "  text = word_tokenize(text)\n",
        "  return text\n",
        "\n",
        "def slang_word(text):\n",
        "  filtered = []\n",
        "  for txt in text:\n",
        "    if txt not in slang_words.keys():\n",
        "      filtered.append(txt)\n",
        "    if txt in slang_words.keys():\n",
        "      x = txt.replace(txt, slang_words[txt])\n",
        "      filtered.append(x)\n",
        "  text = filtered\n",
        "  return text\n",
        "\n",
        "def stemmingText(text):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "  return text\n",
        "\n",
        "def toSentence(list_words):\n",
        "  sentence = ' '.join(word for word in list_words)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(list):\n",
        "  cleaned_text = [remove_punctuation(x) for x in list]\n",
        "  cleaned_text = [case_folding(x) for x in cleaned_text]\n",
        "  tokenized_text = [tokenizingText(x) for x in cleaned_text]\n",
        "  standarized_text = [slang_word(x) for x in tokenized_text]\n",
        "  stemmed_text = [stemmingText(x) for x in standarized_text]\n",
        "  preprocessed_text = [toSentence(x) for x in tokenized_text]\n",
        "\n",
        "  return preprocessed_text"
      ],
      "metadata": {
        "id": "_SVDN07_vZXS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Function"
      ],
      "metadata": {
        "id": "VhrxBgXs7ZFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(split=' ', oov_token='<OOV>')\n",
        "with open('word_index.json') as json_word_index:\n",
        "  word_index = json.load(json_word_index)\n",
        "tokenizer.word_index = word_index"
      ],
      "metadata": {
        "id": "_qDT0sExwoYf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_for_sequences(preprocessed_text):\n",
        "  sequences = tokenizer.texts_to_sequences(preprocessed_text)\n",
        "  padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "id": "nlthNO95w849"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Prediction"
      ],
      "metadata": {
        "id": "esh1OMdc7hNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('bidirectional_lstm.h5')"
      ],
      "metadata": {
        "id": "azwTC-ESyUmq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_to_predict(text):\n",
        "  preprocessed_text = data_preprocessing(text)\n",
        "  padded_sequences = token_for_sequences(preprocessed_text)\n",
        "  result = model.predict(padded_sequences)\n",
        "  weight = 0\n",
        "  denominator = 0\n",
        "  for value in result:\n",
        "    weight +=1\n",
        "    value *= weight\n",
        "    denominator+=weight\n",
        "  avg_result = np.sum(result)/denominator\n",
        "  return avg_result"
      ],
      "metadata": {
        "id": "R7QFXjeUywdK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "6A25EPyCeg9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_to_predict(['Sangat agresif saat melihat anjing lain; Saya takut berantem dengan anjing lain.',\n",
        "                       'Ketika aku memanggilnya untuk makan, Henry akan datang dengan cepat, mengibaskan ekornya. Dia akan duduk dengan sabar menunggu sinyal dariku, dan kemudian menikmati makanannya dengan lahap.',\n",
        "                       'Hari ini anjingku tidak lagi kencing sembarangan dan saya senang sekali',\n",
        "                       'Anjingku tidak nafsu makan, meskipun dia masih sering bermain dengan teman-temannya',\n",
        "                       'anjingku benar-benar malas banget, dia cuma tidur-tiduran tidak karuan. AKu juga sudah mencoba membawanya berkeliling, tapi dia kelihatan gak semangat.'])"
      ],
      "metadata": {
        "id": "wWQW3Ceazo16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a66073-dd25-4bd0-e0cb-55d05c14b8d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5746988932291667"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}