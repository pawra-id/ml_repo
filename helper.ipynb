{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "import json\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ot7iZput-x",
        "outputId": "9bbee88c-ad56-4524-b030-a57405a9bb0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "rEMxhoNMvA05"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsmLhViyu9Rn",
        "outputId": "5673a121-4060-4baf-8485-1d841bac8bc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Function"
      ],
      "metadata": {
        "id": "tPIRypk7uj6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"slang_words.txt\", \"r\") as slang_file:\n",
        "  slang_content = slang_file.read()\n",
        "  slang_words = ast.literal_eval(slang_content)"
      ],
      "metadata": {
        "id": "h67rEp0SYn60"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EKoeFyH0uSqm"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "  text = re.sub('-',' ',text)\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text\n",
        "\n",
        "def case_folding(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "  text = word_tokenize(text)\n",
        "  return text\n",
        "\n",
        "def slang_word(text):\n",
        "  filtered = []\n",
        "  for txt in text:\n",
        "    if txt not in slang_words.keys():\n",
        "      filtered.append(txt)\n",
        "    if txt in slang_words.keys():\n",
        "      x = txt.replace(txt, slang_words[txt])\n",
        "      filtered.append(x)\n",
        "  text = filtered\n",
        "  return text\n",
        "\n",
        "def stemmingText(text):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "  return text\n",
        "\n",
        "def toSentence(list_words):\n",
        "  sentence = ' '.join(word for word in list_words)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(list):\n",
        "  cleaned_text = [remove_punctuation(x) for x in list]\n",
        "  cleaned_text = [case_folding(x) for x in cleaned_text]\n",
        "  tokenized_text = [tokenizingText(x) for x in cleaned_text]\n",
        "  standarized_text = [slang_word(x) for x in tokenized_text]\n",
        "  stemmed_text = [stemmingText(x) for x in standarized_text]\n",
        "  preprocessed_text = [toSentence(x) for x in tokenized_text]\n",
        "\n",
        "  return preprocessed_text"
      ],
      "metadata": {
        "id": "_SVDN07_vZXS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Function"
      ],
      "metadata": {
        "id": "VhrxBgXs7ZFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(split=' ', oov_token='<OOV>')\n",
        "with open('word_index.json') as json_word_index:\n",
        "  word_index = json.load(json_word_index)\n",
        "tokenizer.word_index = word_index"
      ],
      "metadata": {
        "id": "_qDT0sExwoYf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_for_sequences(preprocessed_text):\n",
        "  sequences = tokenizer.texts_to_sequences(preprocessed_text)\n",
        "  padded_sequences = pad_sequences(sequences, padding='post', maxlen=100)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "id": "nlthNO95w849"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Prediction"
      ],
      "metadata": {
        "id": "esh1OMdc7hNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('bidirectional_lstm.h5')"
      ],
      "metadata": {
        "id": "azwTC-ESyUmq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_to_predict(text):\n",
        "  preprocessed_text = data_preprocessing(text)\n",
        "  padded_sequences = token_for_sequences(preprocessed_text)\n",
        "  result = model.predict(padded_sequences)\n",
        "  avg_result = np.mean(result)\n",
        "  return avg_result\n"
      ],
      "metadata": {
        "id": "R7QFXjeUywdK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "6A25EPyCeg9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_to_predict(['Sangat agresif saat melihat anjing lain; Saya takut berantem dengan anjing lain.',\n",
        "                       'Hari ini anjingku tidak lagi kencing sembarangan dan saya senang sekali',\n",
        "                     'Anjingku tidak nafsu makan, meskipun dia masih sering bermain dengan teman-temannya',])"
      ],
      "metadata": {
        "id": "wWQW3Ceazo16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6878cbae-f48b-45a5-ddf2-31443ce06de1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5360802"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}